{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nUGfn_G3qcLV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **working with Twitter data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H02ZoRcGtpaY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 1:  Loading the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VWhc-_nNtsnT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you want to load the results you have previously saved, simply execute the next code, specifying the path to the file.\n",
    "\n",
    "You will need to either upload it to the Colab workspace or copy the path to the file on Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = \"D:/other/job/students_project/network_science/maryam_project/data/gymnastics/\"\n",
    "filterPath = \"D:/other/job/students_project/network_science/maryam_project/data/gymnastics/filtered_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_pickle(path+\"tweets.pkl\")\n",
    "tweets_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ItAgz7uga9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 2: Preprocessing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WSvM1_xzu9Uc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In our dataframe we have the entire Tweet object. Some columns that might be of particular interest to us are: \n",
    "\n",
    "*   created_at - date when Tweet was posted\n",
    "*   id - unique Tweet identifier\n",
    "*   text - the content of the Tweet\n",
    "*   author_id - unique Tweet identifier\n",
    "*   retweeted_status  - information about the original Tweet\n",
    "*   public metrics - quote/reply/retweet/favorite count\n",
    "*   entities - hashtags, urls, annotations present in Tweet\n",
    "\n",
    "We can filter the dataframe and keep only columns we are interested in. You can pick which columns you'd like to keep and put them int the column_list below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets_filtered = tweets_df.copy()\n",
    "\n",
    "tweets_filtered.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hPgzBUJj0SZU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 3: Extracting words/hashtags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CygaLHAS0nzP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are many ways to build networks from the data we download from Twitter.\n",
    "\n",
    "One possibility is to have a bipartite network of Tweets and words/hashtags and then observe word, hashtag or word-hashtag projections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H1nTCbRc0-__",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8pGnokgK1jma",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to extract words, we first need to clean the Tweet text. This way we will remove punctuation, hashtags/mentions/urls (they are preserved in the entity column anyway). We will also turn all letters to lowercase.\n",
    "\n",
    "You can also consider removing stopwords, removing words that are not in the english language corpora, lematizing the words, etc. I suggest you research nltk library and its possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5746Mq918dG",
    "outputId": "f9d8292c-0b46-4ee8-9124-aff7d232edf0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# NLTK tools\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8Nrv5jv1e5W",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cleaner(tweet):\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) # remove mentions\n",
    "    tweet = re.sub(\"#[A-Za-z0-9]+\", \"\",tweet) # remove hashtags\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) if w.lower() in words and not w.lower() in stop_words)\n",
    "     #remove stop words\n",
    "    lemma_function = WordNetLemmatizer()\n",
    "    tweet = \" \".join(lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(tweet))) #lemmatize\n",
    "    tweet = str.lower(tweet) #to lowercase\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEhs-tyy2naZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets_filtered[\"clean_text\"] = tweets_filtered[\"text\"].map(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TPcEWnQuco41",
    "outputId": "2ad93404-af63-46cb-c24e-d85f5af6df4a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I1_lSQZ85rT_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are going to loop through the dataframe and then through the words in the clean text. We are going to add the words as keys to dictionary and use their frequencies as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8paxTEfe1se",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets_filtered.loc[tweets_filtered[\"clean_text\"].isnull(),\"clean_text\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uniqueTweets = tweets_filtered.copy()\n",
    "uniqueTweets = uniqueTweets.drop_duplicates('clean_text')\n",
    "uniqueTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlHlvVlua3bq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet_tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "#initialize an empty dict\n",
    "unique_words = {}\n",
    "\n",
    "for idx, row in uniqueTweets.iterrows():\n",
    "  if row[\"clean_text\"] != \"\":\n",
    "    for word in tweet_tokenizer.tokenize(row[\"clean_text\"]):\n",
    "      unique_words.setdefault(word,0)\n",
    "      unique_words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgMjaVe-a3bt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uw_df = pd.DataFrame.from_dict(unique_words, orient='index').reset_index()\n",
    "uw_df.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
    "uw_df.sort_values(by=['Count'], ascending=False, inplace=True)\n",
    "uw_df = uw_df.reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kVYvDBQu57If",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can inspect the words as a dataframe. \n",
    "\n",
    "\n",
    "You can always save this dataframe as .csv for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "eZW8-9gYbHJk",
    "outputId": "d32e1447-b2e3-4224-f6fb-bd60959e4304",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJ-eBTJWbH-r",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uw_df.to_csv(path+\"words.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "C4R-j-FN7Rgo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the hashtags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O8SfJwmf9GU2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are going to loop through the dataframe and then through the hashtags in the entities. We are going to add the hashtags as keys to dictionary and use their frequencies as values. At the same time, we are going to save them in a list and add them to a separate column to facilitate our future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ph7grrfEX_kl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uniqueTweets.loc[tweets_df[\"entities\"].isnull(), \"entities\"] = None\n",
    "uniqueTweets[\"hashtags\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_PVYuyase5Y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_hashtags = {}\n",
    "index = 0\n",
    "\n",
    "for idx, row in uniqueTweets.iterrows():\n",
    "  if row[\"entities\"] is not None and \"hashtags\" in row[\"entities\"]:\n",
    "    hl = []\n",
    "    for hashtag in row[\"entities\"][\"hashtags\"]:\n",
    "      tag = \"#\" + hashtag[\"tag\"].lower()\n",
    "      unique_hashtags.setdefault(tag, 0)\n",
    "      unique_hashtags[tag] += 1\n",
    "      hl.append(tag)\n",
    "\n",
    "    uniqueTweets.at[idx,\"hashtags\"] = hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPMRJH8wX2rF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_hashtags = dict(sorted(unique_hashtags.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tw1AqDdvX2rH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uh_df = pd.DataFrame.from_dict(unique_hashtags, orient='index').reset_index()\n",
    "uh_df.rename(columns = {'index':'Hashtag', 0:'Count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nx4klPFuX2rI",
    "outputId": "63fdc7d9-79c8-4f89-a087-c389c51017fd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uh_df[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCb7arJqUXOu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uh_df.to_csv(path+\"hashtags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets_filtered.to_pickle(path + \"filtered_data/tweets-filtered.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "type(tweets_df.at[35, 'clean_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wPaVj1tj9Uw6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 4: Building the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vt2co2ep9YCd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are going to use the networkx library, which is a Python library that enables network science analysis of the data.\n",
    "\n",
    "We are going to use it to create our network and extract edgelist from it, since we can easily import it to Gephi (a software we are going to see in visualization labs).\n",
    "\n",
    "However, it offers implemented algorithms for analysis (for example PageRank) that you can use out-of-box to analyze your network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gnd62ng6-GLW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "But first, we will loop through our dataframe and connect words and hashtags if they appear together in the same Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BooMyc6-1JWa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adLbCz86M7SR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "uh = unique_hashtags.keys()\n",
    "uw = unique_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHuQ3rRXOA5_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network = {}\n",
    "network_key = 0\n",
    "for index, row in uniqueTweets.iterrows():\n",
    "    combined_list = [hashtag for hashtag in row[\"hashtags\"]] + [word for word in str.split(row[\"clean_text\"], \" \") if word in uw]\n",
    "    #itertool product creates Cartesian product of each element in the combined list\n",
    "    for pair in itertools.product(combined_list, combined_list):\n",
    "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
    "        if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
    "            network.setdefault(pair,0)\n",
    "            network[pair] += 1 \n",
    "    \n",
    "network_df = pd.DataFrame.from_dict(network, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8uThrYGHSdEe",
    "outputId": "6c53e245-456b-4f28-bc65-eef6d724ae88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network_df.reset_index(inplace=True)\n",
    "network_df.columns = [\"pair\",\"weight\"]\n",
    "network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJvNvzGXy8Kg",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#to get weighted graph we need a list of 3-element tuplels (u,v,w) where u and v are nodes and w is a number representing weight\n",
    "up_weighted = []\n",
    "for edge in network:\n",
    "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
    "    #if(network[edge])>1:\n",
    "    up_weighted.append((edge[0],edge[1],network[edge]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(up_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSneLIqZNvt1",
    "outputId": "a786867e-105b-45da-8a63-fbf8ae8bddff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(G.nodes()))\n",
    "print(len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G,path+\"network.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj3CwR5Cy8Kk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFtpm869ONHg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = path+\"edgelist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTmGSBc3y8Kn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nx.write_weighted_edgelist(G, filename, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "headerList = ['Source', 'Target', 'Weight']\n",
    "file = pd.read_csv(filename)\n",
    "file.to_csv(filename, header=headerList, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e5oT2lSry8Kq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create and save node list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "lpef5RKvUu_w",
    "outputId": "7daf8083-d1c2-45b8-8923-e1d23b300477",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_nodes = pd.DataFrame.from_dict(unique_words,orient=\"index\")\n",
    "word_nodes.reset_index(inplace=True)\n",
    "word_nodes[\"Label\"] = word_nodes[\"index\"]\n",
    "word_nodes.rename(columns={\"index\":\"Id\",0:\"delete\"},inplace=True)\n",
    "word_nodes = word_nodes.drop(columns=['delete'])\n",
    "\n",
    "word_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ZMdIcS4my8Ks",
    "outputId": "9caa2d0a-e50d-4f56-b086-91b6d6c09723",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hashtag_nodes = uh_df.copy()\n",
    "hashtag_nodes[\"Label\"] = hashtag_nodes[\"Hashtag\"]\n",
    "hashtag_nodes.rename(columns={\"Hashtag\":\"Id\"},inplace=True)\n",
    "hashtag_nodes = hashtag_nodes.drop(columns=['Count'])\n",
    "hashtag_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpE0P0cIU2OD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nodelist = pd.concat([hashtag_nodes, word_nodes], ignore_index=True)\n",
    "nodelist.to_csv(path+\"nodelist.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ryR3LiMVB-70",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tasks: \n",
    "\n",
    "*  We created a network where nodes are mixed (both words and hashtags). Create network of words only and one of hashtags only.\n",
    "* Pick one of these network and rank the nodes using PageRank centrality. Extract information about top-20 rated nodes.\n",
    "* following the procedure for extracting hashtags, extract mentions and annotations\n",
    "* following the same procedure, extract the public metric counts for tweets\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "ffccf19f5bea3c345c854d505af45eafe6890432f50bcc58a653a75b25e4385b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
